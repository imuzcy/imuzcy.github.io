---
layout: post
title: NLP相关工具安装
date: 2018-08-01
tags: [NLP,工具]
---

<h1>NLP相关工具安装</h1>

<h2>1.使用Stanford CoreNLP的Python封装包处理中文（分词、词性标注、命名实体识别、句法树、依存句法分析）</h2>
>1.安装stanfordcorenlp
>- 1) 下载安装JDK1.8及以上版本.
>- 2) 下载StanfordCoreNLP文件,解压.
>- 3) 处理模型下载jar文件,然后放到stanford-corenlp-full-xxxx-xx-xx根目录下即可

**pip命令:**
``` python
sudo pip install stanfordcorenlp
```

<h3>Python-demo:</h3>
``` python
# coding=utf-8
from stanfordcorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP(r'/home/crius/stanford-corenlp-full-2018-02-27', lang='zh')

sentence = '清华大学位于北京。'
print(nlp.word_tokenize(sentence))
print(nlp.pos_tag(sentence))
print(nlp.ner(sentence))
print(nlp.parse(sentence))
print(nlp.dependency_parse(sentence))
```

<h3>故障排除</h3>
模型初始化通常需要几十秒，如果很久不出结果，加入quiet=False, logging_level=logging.DEBUG参数输出Debug信息：
``` python
import logging
from stanfordcorenlp import StanfordCoreNLP

nlp = StanfordCoreNLP(r'path_or_host', quiet=False, logging_level=logging.DEBUG)
```

如果出现port is in use的出错信息，换一个端口就好了：
``` python
nlp = StanfordCoreNLP(r'/home/crius/stanford-corenlp-full-2016-10-31/', port=9001, lang='zh')
```

<h2>2.安装哈工大LTP</h2>

**1.pip安装**
``` python
pip install pyltp
```
**2.下载模型文件,[七牛云](https://github.com/HIT-SCIR/pyltp),当前模型版本 3.4.0**.


<h3>Python-demo:</h3>
``` python
# -*- coding: utf-8 -*-
from pyltp import Segmentor
segmentor = Segmentor()
segmentor.load("/home/crius/ltp_data_v3.4.0/cws.model")
words = segmentor.segment("元芳你怎么看")
print("|".join(words))
segmentor.release()
```
除了分词之外，pyltp 还提供词性标注、命名实体识别、依存句法分析、语义角色标注等功能。
详细使用方法请参考 [在线文档](http://pyltp.readthedocs.io/zh_CN/latest/)


<h2>3.结巴分词</h2>

**1.pip安装**
``` python
pip install jieba
```
